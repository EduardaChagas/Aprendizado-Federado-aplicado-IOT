{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bj3F1nTZj1ho"
   },
   "source": [
    "# Hands On - Aprendizado Federado aplicado à Internet das Coisas\n",
    "\n",
    "**Notebook 2**: Criação de clientes no ambiente federado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cJPTVnswkMdN"
   },
   "source": [
    "O reconhecimento da atividade humana é uma área de pesquisa ativa e que possui um enorme potencial de benefício com o uso de aprendizado federado (FL), já que tais dados são normalmente privados e possuem informações sensíveis sobre os usuários.\n",
    "Além disso, com FL também podemos desenvolver um modelo conjunto que consiga capturar a diversidade dos dados, algo que é extremamente difícil de ser coletado de forma individual.\n",
    "\n",
    "Sob esse contexto, nesse tutorial vamos aprender como definir clientes para o treinamento federado de uma rede neural para auxilar no reconhecimento de atividades humanas (*Human Activity Recognition* - HAR) usando o framework de aprendizado federado\n",
    "Flower em conjunto com a biblioteca de deep learning Pytorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2hX7rxsAk8CT"
   },
   "source": [
    "### Dataset\n",
    "\n",
    "Os dados serão particionados horizontalmente, assim os subconjuntos de treinamento e teste irão ser divididos em mini-batches (pequenos lotes) com base no número total de clientes.\n",
    "\n",
    "Para isso, aplicaremos uma função auxiliar para carregar os dados e definir os conjuntos de treinamento e teste.\n",
    "Nessa função, precisaremos dos seguintes parâmetros: \n",
    "\n",
    "* **data root (str)**: Diretório onde os datasets finais serão armazenados. \n",
    "\n",
    "* **train batch size (int)**: Tamanho do mini-batch usado nos dados de treinamento.\n",
    "\n",
    "* **test batch size (int)**: Tamanho do mini-batch usado nos dados de teste. \n",
    "\n",
    "* **id (int)**: Client ID usado para selecionar uma partição específica. \n",
    "\n",
    "* **nb clients (int)**: Número total de clientes usados no treinamento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UVQzVKL2r67J",
    "outputId": "67066d3f-f7bb-412c-e76b-091aea68ff3c"
   },
   "outputs": [],
   "source": [
    "#Carregando os dados\n",
    "import flwr as fl\n",
    "import torch\n",
    "import aux\n",
    "\n",
    "DATA_ROOT = \"./data/pml-training.csv\"\n",
    "\n",
    "server_address = \"[::]:8081\"\n",
    "cid = 0\n",
    "nb_clients = 2\n",
    "train_batch_size = 64\n",
    "test_batch_size = 64\n",
    "epochs = 10\n",
    "\n",
    "# Load data\n",
    "train_loader, test_loader = aux.load_data(\n",
    "        data_root = DATA_ROOT,\n",
    "        train_batch_size = train_batch_size,\n",
    "        test_batch_size = test_batch_size,\n",
    "        cid = cid,\n",
    "        nb_clients = nb_clients + 1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Arvhg44xrwWm"
   },
   "source": [
    "### Rede Neural\n",
    "\n",
    "Atualmente o modelo de classificação mais adequado e vantajoso para a modelagem de um ambiente federado são as redes neurais.\n",
    "Definimos essa configuração de arquitetura por meio da criação de uma classe em Pytorch denominada **HARmodel** presente no arquivo auxiliar *aux.py* adicionado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-glqWQnYsGt3"
   },
   "source": [
    "### Cliente Flower\n",
    "\n",
    "O próximo passo é definir a alocação dos dispositivos no ambiente federado. \n",
    "\n",
    "Quando o servidor seleciona um dispositivo específico do ambiente federado para realizar um treinamento, ele envia as instruções pela rede, por meio de uma interface chamada **Client**.\n",
    "Assim, o cliente recebe as instruções do servidor e chama um dos métodos desta classe para executar seu código (ou seja, para treinar a sua rede neural local). \n",
    "\n",
    "O framework Flower fornece uma classe chamada *NumPyClient*, que torna mais fácil implementar a interface do cliente quando utilizamos PyTorch. \n",
    "Quando implementamos um NumPyClient devemos definir os seguintes métodos: \n",
    "\n",
    "* **get_parameters**: retorna o peso do modelo\n",
    "como uma lista de ndarrays \n",
    "\n",
    "* **set_parameters** (opcional): atualiza os pesos do modelo\n",
    "local com os parâmetros recebidos do servidor \n",
    "\n",
    "* **fit**: define os pesos do modelo local, treina o modelo localmente e recebe o update dos pesos locais \n",
    "\n",
    "* **evaluate**: define como o modelo local será testado. \n",
    "\n",
    "Abaixo mostramos como a classe Client foi implementada\n",
    "para o caso de estudo apresentado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "MEUIciNJ69re"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Instantiate client\n",
    "client = aux.FlowerClient(\n",
    "    cid = cid,\n",
    "    train_loader = train_loader,\n",
    "    test_loader = test_loader,\n",
    "    epochs = epochs,\n",
    "    device = device,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yNxtUr3s67hn"
   },
   "source": [
    "### Inicializando o cliente\n",
    "\n",
    "O flower nos fornece a possibilidade de rodar o servidor e o cliente na mesma máquina, configurando o endereço do servidor como \"[::]: 8080\". \n",
    "Porém, se quisermos implementar uma aplicação realmente federada com o servidor e clientes em execução em diferentes máquinas, precisaremos apenas alterar o server address para o respectivo endereço da máquina do cliente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "CD9ie8II7QHB"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG flower 2021-08-19 22:31:09,178 | connection.py:36 | ChannelConnectivity.IDLE\n",
      "DEBUG flower 2021-08-19 22:31:09,180 | connection.py:36 | ChannelConnectivity.CONNECTING\n",
      "DEBUG flower 2021-08-19 22:31:09,181 | connection.py:36 | ChannelConnectivity.READY\n",
      "INFO flower 2021-08-19 22:31:09,181 | app.py:61 | Opened (insecure) gRPC connection\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 10 epoch(s) w/ 103 mini-batches each\n",
      "\n",
      "Train Epoch: 0 [6336/6592 (96%)] Loss: 1.624247, Acc: 0.237216 (Cliente 0)\t\t\t\t\n",
      "Train Epoch: 1 [6336/6592 (96%)] Loss: 1.559280, Acc: 0.286774 (Cliente 0)\t\t\t\t\n",
      "Train Epoch: 2 [6336/6592 (96%)] Loss: 1.348555, Acc: 0.349274 (Cliente 0)\t\t\t\t\n",
      "Train Epoch: 3 [6336/6592 (96%)] Loss: 1.277884, Acc: 0.405777 (Cliente 0)\t\t\t\t\n",
      "Train Epoch: 4 [6336/6592 (96%)] Loss: 1.354746, Acc: 0.441288 (Cliente 0)\t\t\t\t\n",
      "Train Epoch: 5 [6336/6592 (96%)] Loss: 1.200371, Acc: 0.465120 (Cliente 0)\t\t\t\t\n",
      "Train Epoch: 6 [6336/6592 (96%)] Loss: 1.140836, Acc: 0.493371 (Cliente 0)\t\t\t\t\n",
      "Train Epoch: 7 [6336/6592 (96%)] Loss: 1.400344, Acc: 0.511679 (Cliente 0)\t\t\t\t\n",
      "Train Epoch: 8 [6336/6592 (96%)] Loss: 1.115530, Acc: 0.530934 (Cliente 0)\t\t\t\t\n",
      "Training 10 epoch(s) w/ 103 mini-batches each06, Acc: 0.544192 (Cliente 0)\t\t\t\t\n",
      "\n",
      "Train Epoch: 0 [6336/6592 (96%)] Loss: 1.074452, Acc: 0.524463 (Cliente 0)\t\t\t\t\n",
      "Train Epoch: 1 [6336/6592 (96%)] Loss: 1.132696, Acc: 0.546402 (Cliente 0)\t\t\t\t\n",
      "Train Epoch: 2 [6336/6592 (96%)] Loss: 0.991205, Acc: 0.563289 (Cliente 0)\t\t\t\t\n",
      "Train Epoch: 3 [6336/6592 (96%)] Loss: 1.194289, Acc: 0.570707 (Cliente 0)\t\t\t\t\n",
      "Train Epoch: 4 [6336/6592 (96%)] Loss: 1.017011, Acc: 0.567708 (Cliente 0)\t\t\t\t\n",
      "Train Epoch: 5 [6336/6592 (96%)] Loss: 0.942879, Acc: 0.580966 (Cliente 0)\t\t\t\t\n",
      "Train Epoch: 6 [6336/6592 (96%)] Loss: 1.025218, Acc: 0.594855 (Cliente 0)\t\t\t\t\n",
      "Train Epoch: 7 [6336/6592 (96%)] Loss: 0.898544, Acc: 0.603535 (Cliente 0)\t\t\t\t\n",
      "Train Epoch: 8 [6336/6592 (96%)] Loss: 1.009436, Acc: 0.600694 (Cliente 0)\t\t\t\t\n",
      "Training 10 epoch(s) w/ 103 mini-batches each63, Acc: 0.602273 (Cliente 0)\t\t\t\t\n",
      "\n",
      "Train Epoch: 0 [6336/6592 (96%)] Loss: 0.917539, Acc: 0.593434 (Cliente 0)\t\t\t\t\n",
      "Train Epoch: 1 [6336/6592 (96%)] Loss: 1.017181, Acc: 0.613636 (Cliente 0)\t\t\t\t\n",
      "Train Epoch: 2 [6336/6592 (96%)] Loss: 0.993769, Acc: 0.613952 (Cliente 0)\t\t\t\t\n",
      "Train Epoch: 3 [6336/6592 (96%)] Loss: 0.924528, Acc: 0.620581 (Cliente 0)\t\t\t\t\n",
      "Train Epoch: 4 [6336/6592 (96%)] Loss: 0.945030, Acc: 0.615372 (Cliente 0)\t\t\t\t\n",
      "Train Epoch: 5 [6336/6592 (96%)] Loss: 1.119702, Acc: 0.627683 (Cliente 0)\t\t\t\t\n",
      "Train Epoch: 6 [6336/6592 (96%)] Loss: 1.029284, Acc: 0.635101 (Cliente 0)\t\t\t\t\n",
      "Train Epoch: 7 [6336/6592 (96%)] Loss: 0.899909, Acc: 0.636679 (Cliente 0)\t\t\t\t\n",
      "Train Epoch: 8 [6336/6592 (96%)] Loss: 0.991709, Acc: 0.638889 (Cliente 0)\t\t\t\t\n",
      "Training 10 epoch(s) w/ 103 mini-batches each17, Acc: 0.639362 (Cliente 0)\t\t\t\t\n",
      "\n",
      "Train Epoch: 0 [6336/6592 (96%)] Loss: 0.826196, Acc: 0.647254 (Cliente 0)\t\t\t\t\n",
      "Train Epoch: 1 [6336/6592 (96%)] Loss: 0.843055, Acc: 0.648516 (Cliente 0)\t\t\t\t\n",
      "Train Epoch: 2 [6336/6592 (96%)] Loss: 1.026695, Acc: 0.655934 (Cliente 0)\t\t\t\t\n",
      "Train Epoch: 3 [6336/6592 (96%)] Loss: 0.902703, Acc: 0.657828 (Cliente 0)\t\t\t\t\n",
      "Train Epoch: 4 [6336/6592 (96%)] Loss: 0.887367, Acc: 0.667456 (Cliente 0)\t\t\t\t\n",
      "Train Epoch: 5 [6336/6592 (96%)] Loss: 0.995443, Acc: 0.670455 (Cliente 0)\t\t\t\t\n",
      "Train Epoch: 6 [6336/6592 (96%)] Loss: 0.772683, Acc: 0.669665 (Cliente 0)\t\t\t\t\n",
      "Train Epoch: 7 [6336/6592 (96%)] Loss: 0.715607, Acc: 0.665878 (Cliente 0)\t\t\t\t\n",
      "Train Epoch: 8 [6336/6592 (96%)] Loss: 1.051720, Acc: 0.670455 (Cliente 0)\t\t\t\t\n",
      "Training 10 epoch(s) w/ 103 mini-batches each32, Acc: 0.667771 (Cliente 0)\t\t\t\t\n",
      "\n",
      "Train Epoch: 0 [6336/6592 (96%)] Loss: 0.790817, Acc: 0.672822 (Cliente 0)\t\t\t\t\n",
      "Train Epoch: 1 [6336/6592 (96%)] Loss: 0.816631, Acc: 0.688920 (Cliente 0)\t\t\t\t\n",
      "Train Epoch: 2 [6336/6592 (96%)] Loss: 0.957397, Acc: 0.681818 (Cliente 0)\t\t\t\t\n",
      "Train Epoch: 3 [6336/6592 (96%)] Loss: 0.771906, Acc: 0.677557 (Cliente 0)\t\t\t\t\n",
      "Train Epoch: 4 [6336/6592 (96%)] Loss: 0.782507, Acc: 0.682449 (Cliente 0)\t\t\t\t\n",
      "Train Epoch: 5 [6336/6592 (96%)] Loss: 0.636797, Acc: 0.681976 (Cliente 0)\t\t\t\t\n",
      "Train Epoch: 6 [6336/6592 (96%)] Loss: 0.782185, Acc: 0.696023 (Cliente 0)\t\t\t\t\n",
      "Train Epoch: 7 [6336/6592 (96%)] Loss: 0.858257, Acc: 0.686237 (Cliente 0)\t\t\t\t\n",
      "Train Epoch: 8 [6336/6592 (96%)] Loss: 0.838281, Acc: 0.689710 (Cliente 0)\t\t\t\t\n",
      "Training 10 epoch(s) w/ 103 mini-batches each91, Acc: 0.692077 (Cliente 0)\t\t\t\t\n",
      "\n",
      "Train Epoch: 0 [6336/6592 (96%)] Loss: 0.860670, Acc: 0.697128 (Cliente 0)\t\t\t\t\n",
      "Train Epoch: 1 [6336/6592 (96%)] Loss: 1.128795, Acc: 0.701389 (Cliente 0)\t\t\t\t\n",
      "Train Epoch: 2 [6336/6592 (96%)] Loss: 0.893592, Acc: 0.706439 (Cliente 0)\t\t\t\t\n",
      "Train Epoch: 3 [6336/6592 (96%)] Loss: 0.796353, Acc: 0.714015 (Cliente 0)\t\t\t\t\n",
      "Train Epoch: 4 [6336/6592 (96%)] Loss: 0.931280, Acc: 0.702967 (Cliente 0)\t\t\t\t\n",
      "Train Epoch: 5 [6336/6592 (96%)] Loss: 0.733448, Acc: 0.709280 (Cliente 0)\t\t\t\t\n",
      "Train Epoch: 6 [6336/6592 (96%)] Loss: 0.788450, Acc: 0.702652 (Cliente 0)\t\t\t\t\n",
      "Train Epoch: 7 [6336/6592 (96%)] Loss: 0.649509, Acc: 0.711648 (Cliente 0)\t\t\t\t\n",
      "Train Epoch: 8 [6336/6592 (96%)] Loss: 0.882347, Acc: 0.704861 (Cliente 0)\t\t\t\t\n",
      "Training 10 epoch(s) w/ 103 mini-batches each46, Acc: 0.708018 (Cliente 0)\t\t\t\t\n",
      "\n",
      "Train Epoch: 0 [6336/6592 (96%)] Loss: 0.876159, Acc: 0.697601 (Cliente 0)\t\t\t\t\n",
      "Train Epoch: 1 [6336/6592 (96%)] Loss: 0.760278, Acc: 0.701862 (Cliente 0)\t\t\t\t\n",
      "Train Epoch: 2 [6336/6592 (96%)] Loss: 0.948556, Acc: 0.710385 (Cliente 0)\t\t\t\t\n",
      "Train Epoch: 3 [6336/6592 (96%)] Loss: 0.738762, Acc: 0.712595 (Cliente 0)\t\t\t\t\n",
      "Train Epoch: 4 [6336/6592 (96%)] Loss: 0.643807, Acc: 0.705177 (Cliente 0)\t\t\t\t\n",
      "Train Epoch: 5 [6336/6592 (96%)] Loss: 0.897479, Acc: 0.710227 (Cliente 0)\t\t\t\t\n",
      "Train Epoch: 6 [6336/6592 (96%)] Loss: 0.869907, Acc: 0.723958 (Cliente 0)\t\t\t\t\n",
      "Train Epoch: 7 [6336/6592 (96%)] Loss: 0.535154, Acc: 0.714015 (Cliente 0)\t\t\t\t\n",
      "Train Epoch: 8 [6336/6592 (96%)] Loss: 0.840426, Acc: 0.717330 (Cliente 0)\t\t\t\t\n",
      "Training 10 epoch(s) w/ 103 mini-batches each48, Acc: 0.711490 (Cliente 0)\t\t\t\t\n",
      "\n",
      "Train Epoch: 0 [6336/6592 (96%)] Loss: 0.880695, Acc: 0.704230 (Cliente 0)\t\t\t\t\n",
      "Train Epoch: 1 [6336/6592 (96%)] Loss: 0.795616, Acc: 0.724747 (Cliente 0)\t\t\t\t\n",
      "Train Epoch: 2 [6336/6592 (96%)] Loss: 0.786563, Acc: 0.723801 (Cliente 0)\t\t\t\t\n",
      "Train Epoch: 3 [6336/6592 (96%)] Loss: 0.979308, Acc: 0.716067 (Cliente 0)\t\t\t\t\n",
      "Train Epoch: 4 [6336/6592 (96%)] Loss: 0.629465, Acc: 0.719381 (Cliente 0)\t\t\t\t\n",
      "Train Epoch: 5 [6336/6592 (96%)] Loss: 0.543995, Acc: 0.717330 (Cliente 0)\t\t\t\t\n",
      "Train Epoch: 6 [6336/6592 (96%)] Loss: 0.757717, Acc: 0.729956 (Cliente 0)\t\t\t\t\n",
      "Train Epoch: 7 [6336/6592 (96%)] Loss: 0.941307, Acc: 0.726168 (Cliente 0)\t\t\t\t\n",
      "Train Epoch: 8 [6336/6592 (96%)] Loss: 0.910131, Acc: 0.722064 (Cliente 0)\t\t\t\t\n",
      "Training 10 epoch(s) w/ 103 mini-batches each46, Acc: 0.719855 (Cliente 0)\t\t\t\t\n",
      "\n",
      "Train Epoch: 0 [6336/6592 (96%)] Loss: 0.680786, Acc: 0.708333 (Cliente 0)\t\t\t\t\n",
      "Train Epoch: 1 [6336/6592 (96%)] Loss: 0.735722, Acc: 0.724905 (Cliente 0)\t\t\t\t\n",
      "Train Epoch: 2 [6336/6592 (96%)] Loss: 0.996489, Acc: 0.728378 (Cliente 0)\t\t\t\t\n",
      "Train Epoch: 3 [6336/6592 (96%)] Loss: 0.794123, Acc: 0.720960 (Cliente 0)\t\t\t\t\n",
      "Train Epoch: 4 [6336/6592 (96%)] Loss: 0.615181, Acc: 0.726799 (Cliente 0)\t\t\t\t\n",
      "Train Epoch: 5 [6336/6592 (96%)] Loss: 0.652130, Acc: 0.730271 (Cliente 0)\t\t\t\t\n",
      "Train Epoch: 6 [6336/6592 (96%)] Loss: 0.630850, Acc: 0.729798 (Cliente 0)\t\t\t\t\n",
      "Train Epoch: 7 [6336/6592 (96%)] Loss: 0.960987, Acc: 0.724432 (Cliente 0)\t\t\t\t\n",
      "Train Epoch: 8 [6336/6592 (96%)] Loss: 1.020542, Acc: 0.733586 (Cliente 0)\t\t\t\t\n",
      "Training 10 epoch(s) w/ 103 mini-batches each15, Acc: 0.728851 (Cliente 0)\t\t\t\t\n",
      "\n",
      "Train Epoch: 0 [6336/6592 (96%)] Loss: 0.691842, Acc: 0.728220 (Cliente 0)\t\t\t\t\n",
      "Train Epoch: 1 [6336/6592 (96%)] Loss: 0.705225, Acc: 0.722696 (Cliente 0)\t\t\t\t\n",
      "Train Epoch: 2 [6336/6592 (96%)] Loss: 0.779265, Acc: 0.732639 (Cliente 0)\t\t\t\t\n",
      "Train Epoch: 3 [6336/6592 (96%)] Loss: 0.679574, Acc: 0.731218 (Cliente 0)\t\t\t\t\n",
      "Train Epoch: 4 [6336/6592 (96%)] Loss: 0.617635, Acc: 0.734533 (Cliente 0)\t\t\t\t\n",
      "Train Epoch: 5 [6336/6592 (96%)] Loss: 2.426671, Acc: 0.727904 (Cliente 0)\t\t\t\t\n",
      "Train Epoch: 6 [6336/6592 (96%)] Loss: 1.108344, Acc: 0.730903 (Cliente 0)\t\t\t\t\n",
      "Train Epoch: 7 [6336/6592 (96%)] Loss: 0.760034, Acc: 0.736585 (Cliente 0)\t\t\t\t\n",
      "Train Epoch: 8 [6336/6592 (96%)] Loss: 0.789762, Acc: 0.731692 (Cliente 0)\t\t\t\t\n",
      "Train Epoch: 9 [6336/6592 (96%)] Loss: 0.839387, Acc: 0.732797 (Cliente 0)\t\t\t\t\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG flower 2021-08-19 22:32:15,455 | connection.py:68 | Insecure gRPC channel closed\n",
      "INFO flower 2021-08-19 22:32:15,470 | app.py:72 | Disconnect and shut down\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Client 0 - Evaluate on 6540 samples: Average loss: 0.0080, Accuracy: 82.80%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Código de instanciação do cliente \n",
    "fl.client.start_client(server_address, client)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "2-HAR-client_1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
